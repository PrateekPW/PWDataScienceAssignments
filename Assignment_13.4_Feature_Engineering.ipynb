{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4520ae-0462-4420-8188-4d6dd6743315",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a15b0e-4f3e-4018-9acd-cc5b563a10eb",
   "metadata": {},
   "source": [
    "Ans\n",
    "\n",
    "Min-Max scaling is a technique used in data preprocessing to scale numeric features to a specific range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the range (the maximum value minus the minimum value). This ensures that all the features are proportionally scaled to fit within the specified range. Min-Max scaling is particularly useful when working with algorithms that require input features to be on a similar scale.\n",
    "\n",
    "Example:\n",
    "Suppose you have a feature \"age\" with values ranging from 20 to 60. To apply Min-Max scaling, you would use the following formula:\n",
    "Scaled Age=(age-min_age)/(max_age-min_age)\n",
    "If the age is 30, and the minimum and maximum ages in the dataset are 20 and 60 respectively, then the scaled age would be:\n",
    "Scaled Age=(30-20)/(60-20)=0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72db4f-0299-46da-b2b5-0b94191db107",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25b82d-d979-4ba7-ade6-83cc188ef1ea",
   "metadata": {},
   "source": [
    "Ans\n",
    "\n",
    "The Unit Vector technique in feature scaling scales each feature such that the magnitude of each feature vector is 1. It differs from Min-Max scaling in that it normalizes the vectors to unit length, irrespective of the range of the values.\n",
    "\n",
    "Example:\n",
    "Consider a dataset where you have two features, \"height\" and \"weight\". To apply Unit Vector scaling, you would divide each feature vector by its Euclidean length (magnitude).\n",
    "\n",
    "Unit Vector=(Feature Vector)/|Feature Vector|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ee63e-724d-4b3b-8010-966c2c4bbc82",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b3303-cdd7-48bb-a7ff-54c9aabb5c3b",
   "metadata": {},
   "source": [
    "Ans\n",
    "\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving most of the data's variance. It achieves this by identifying the principal components, which are orthogonal vectors that represent the directions of maximum variance in the data.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with multiple correlated features like height, weight, and age. PCA would transform these features into a new set of uncorrelated variables called principal components, where each successive component captures as much of the remaining variance as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a71635-72cf-4063-88f5-94fd8041bfae",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5bf7c-38cb-41ce-a8ce-6048e45e3027",
   "metadata": {},
   "source": [
    "Ans\n",
    "\n",
    "PCA is closely related to feature extraction as it aims to reduce the dimensionality of the feature space while retaining most of the information. PCA can be used for feature extraction by selecting a subset of the principal components that explain the majority of the variance in the data.\n",
    "\n",
    "Example:\n",
    "In a dataset containing multiple features representing different aspects of a customer's behavior, PCA can be applied to extract a smaller set of principal components that summarize the most significant patterns in the data, such as spending habits, frequency of purchases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497d66f-6e42-4943-87bf-a6aadf5f8ba5",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa2bc8-6826-486d-b555-51275fd6c1fa",
   "metadata": {},
   "source": [
    "Ans\n",
    "\n",
    "To preprocess the data for the food delivery recommendation system using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "Identify numeric features such as price, rating, and delivery time.\n",
    "Compute the minimum and maximum values for each feature.\n",
    "Apply the Min-Max scaling formula to scale each feature to the range [0, 1].\n",
    "Use the scaled features for further analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99944a-a525-467d-be78-f5c15027303c",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce14932-7b2d-4e63-9635-4963b513fc2d",
   "metadata": {},
   "source": [
    "Ans\n",
    "\n",
    "To reduce the dimensionality of the stock price prediction dataset using PCA, you would:\n",
    "\n",
    "Standardize the features (subtract mean and divide by standard deviation) to ensure they have a mean of 0 and a standard deviation of 1.\n",
    "Compute the covariance matrix of the standardized features.\n",
    "Perform eigendecomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "Select the top principal components that capture a significant portion of the variance in the data.\n",
    "Project the original data onto the selected principal components to obtain the reduced-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9a80d-ff7f-42db-81f4-9b0480e02638",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410fb2f6-83f3-4916-9e40-1a6c9518254c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Ans\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "# Create MinMaxScaler object\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Scaled Data:\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e1ae3-5094-497f-9245-6bdb4aefcd14",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fa8e3-cc74-474c-ac43-aacca2b87209",
   "metadata": {},
   "source": [
    "Determining the number of principal components to retain in PCA involves a trade-off between dimensionality reduction and preserving sufficient information from the original dataset. Generally, you would aim to retain enough principal components to explain a significant portion of the variance in the data while reducing dimensionality.\n",
    "\n",
    "To decide on the number of principal components to retain, you can use techniques such as scree plots or cumulative explained variance plots. These methods help visualize the explained variance by each principal component and can guide you in selecting an appropriate number of components.\n",
    "\n",
    "Here's a step-by-step process:\n",
    "\n",
    "Standardize the dataset: Before applying PCA, it's essential to standardize the features to have a mean of 0 and a standard deviation of 1. This ensures that features with larger scales do not dominate the principal components.\n",
    "\n",
    "Apply PCA: Perform PCA on the standardized dataset.\n",
    "\n",
    "Calculate explained variance: After applying PCA, you can access the explained variance ratio of each principal component. This ratio represents the proportion of the dataset's variance that lies along each principal component.\n",
    "\n",
    "Decide on the number of components: Plot the cumulative explained variance against the number of components. This plot helps you visualize how much variance is retained as you increase the number of components. Decide on a threshold for the cumulative explained variance that meets your requirements (e.g., retaining 95% of the variance).\n",
    "\n",
    "Choose the number of components: Select the number of principal components that correspond to the chosen threshold.\n",
    "\n",
    "Let's say after plotting the cumulative explained variance, you find that the first three principal components capture 90% of the variance in the dataset. In this case, you might choose to retain these three components.\n",
    "\n",
    "It's crucial to balance dimensionality reduction with information retention. Choosing too few components may result in significant information loss, while retaining too many components defeats the purpose of dimensionality reduction. Therefore, it's essential to consider the specific requirements of your analysis and the trade-offs involved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
